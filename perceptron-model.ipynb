{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"perceptron-model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPhYFaAiUFXPT4jK0n/gZXS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0dEwlMOQKYI","executionInfo":{"status":"ok","timestamp":1655752866417,"user_tz":-180,"elapsed":15427,"user":{"displayName":"Samuel Muthembwa","userId":"17191772529412425551"}},"outputId":"67412580-2166-49c1-8305-560d0e4cc6a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["*** Learning Iterations Started ***\n","\n","*** Learning Iterations Complete ***\n","\n","[1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1\n"," 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1\n"," 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0\n"," 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0\n"," 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1\n"," 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0]\n"]}],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","\n","#Creating a custom Perceptron\n","#Creating a perceptron class\n","\n","class Perceptron: \n","  def __init__ (self, learning_rate = 0.1, epochs = 500):\n","    self.learning_rate = learning_rate\n","    self.iterations = epochs #Number of learning iterations\n","    self.weights = None\n","    self.bias = None\n","    self.unit_function = self.activation_function\n","\n","  def activation_function(self, X):\n","    return np.where(X>=0, 1, 0) #Letting the threshold to be 0 such that if X is greater than 0 then the output is 1 else 0\n","\n","  def fit(self, train_data, train_labels):\n","    n_samples, n_features = train_data.shape\n","\n","    #initializing the weights\n","    self.weights = np.zeros(n_features)\n","    self.bias = 0\n","\n","    #Converting train_labels to 0s and 1s if not the case\n","    y_converted = np.array([1 if i > 0 else 0 for i in train_labels])\n","\n","    for i in range(self.iterations): #Looping through the number of iteration for learning\n","      if i == 0:\n","        print(\"*** Learning Iterations Started ***\\n\")\n","      if i == self.iterations-1:\n","        print(\"*** Learning Iterations Complete ***\\n\")\n","      \n","      for index, x_i in enumerate(train_data):\n","        output  =np.dot(x_i, self.weights)+ self.bias # adding a bias to the Inputs\n","        y = self.unit_function(output) #Passing the input values through the activation function\n","\n","        update = self.learning_rate*(y_converted[index] - y)\n","        self.weights += update*x_i\n","        self.bias += update #Updating the bias\n","\n","  def predict(self, x ):\n","    #Prediction function\n","    output = np.dot(x, self.weights) + self.bias #Multiplies the weights and the input and adds the bias\n","    y = self.unit_function(output) #\n","    return y\n","\n","#Creating a dataset with 1000 samples and with 4 features from the make_blobs() methods\n","X, y = datasets.make_blobs(n_samples = 1000, n_features = 4, centers = 2, cluster_std=1.05, random_state = 2)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) \n","\n","model = Perceptron(learning_rate = 0.1, epochs = 500)\n","model.fit(X_train, y_train) #Training the model \n","predictions = model.predict(X_test) #Predicting the X_test data\n","print(predictions)"]}]}